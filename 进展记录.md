已完成开题，包括相关文献调研；

已进行SAR目标检测相关研究并发表EI会议一篇；

已规划技术路径；

## 2019.01.09

基于matlab进行了基于光学遥感图像的仿真加噪，并移植到py，并对计算速度进行了一定的优化。

对一张256*256的图像进行加噪，matlab计算约0.6秒左右，直接移植到py需要10秒以上，使用numpy库和map函数对循环进行了优化，现在需要约1.8秒左右。可能还需进一步优化。

建立git仓库并提交目前的东西（不包含之前检测相关的研究）。

## 01.13

引入numba对加噪函数进行了编译优化，从原来的1.8秒左右优化到了0.3秒左右。

## 01.14

准备基于Pix2pix模型以及光学遥感数据集进行简单训练。
写完了noise_simul类，还没进行测试，改了一部分dataloader，还没改完。

## 01.15

对noise_simul类进行了测试，结果与matlab仿真不太一样，主要是图像亮度不一样。
进一步测试发现，对图像数据*2后进行显示结果不变。
推测是因为plt.imshow(img)的时候，如果img是浮点数值，则会强行映射到0-1，不论原值范围是多少。

## 6.11

超分辨之后是SSDD训练精度上不去，原因不明。

为了正常读取16bits数据修改了源码读取图像部分，读入结果格式应该是符合原来的模型的，debug脚本也没看出问题。

试用了tf归一化（-1到1），好像原来没改过这个。所以又改回去使用caffe归一化即减去imagenet均值，不归一。重新基于coco预训练模型进行训练。

也可能是tf库版本的问题，原来是基于Keras和tf1的。这样的话只能从coco模型重新训练原ssdd数据，看看精度还能不能上去。如果能上去，那就是可能超分辨其实破坏了某些特征。

还有目前基于原分辨图像迁移的没有乱序，可以乱序试试，以及原来记得一个epoch是10000步，也改回去试试。

要不就恢复读取图像的源码，改图片格式。

不行的话可能得切换到旧版库了，幸好还有当初旧版环境的docker，希望还能用。